{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Laboratório 2 - UFABC - Laurindo dos Santos - 131710114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "ListaPalavras = ['gato', 'elefante', 'rato', 'rato', 'gato']\n",
    "palavrasRDD = sc.parallelize(ListaPalavras, 4)\n",
    "print (type(palavrasRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gatos\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def Plural(palavra):\n",
    "    \"\"\"Adds an 's' to `palavra`.\n",
    "\n",
    "    Args:\n",
    "        palavra (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "\n",
    "    return str.format(palavra + 's')\n",
    "\n",
    "print (Plural('gato'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Plural in module __main__:\n",
      "\n",
      "Plural(palavra)\n",
      "    Adds an 's' to `palavra`.\n",
      "    \n",
      "    Args:\n",
      "        palavra (str): A string.\n",
      "    \n",
      "    Returns:\n",
      "        str: A string with 's' added to it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert Plural('rato')=='ratos', 'resultado incorreto!'\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "pluralRDD = palavrasRDD.map(lambda x: Plural(x))\n",
    "print (pluralRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert pluralRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD = palavrasRDD.map(lambda x: str.format(x + 's'))\n",
    "print (pluralRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gato', 1), ('elefante', 1), ('rato', 1), ('rato', 1), ('gato', 1)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavraPar = palavrasRDD.map(lambda x: (x, 1))\n",
    "print (palavraPar.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert palavraPar.collect() == [('gato',1),('elefante',1),('rato',1),('rato',1),('gato',1)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elefante: [1]\n",
      "rato: [1, 1]\n",
      "gato: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavrasGrupo = palavraPar.groupByKey()\n",
    "for chave, valor in palavrasGrupo.collect():\n",
    "    print ('{0}: {1}'.format(chave, list(valor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(palavrasGrupo.mapValues(lambda x: list(x)).collect()) == [('elefante', [1]), ('gato',[1, 1]), ('rato',[1, 1])],'Valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('gato', 2), ('rato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagemGroup = palavrasGrupo.mapValues(sum).sortByKey()\n",
    "print (contagemGroup.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert list(sorted(contagemGroup.collect()))==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagem = palavraPar.reduceByKey(lambda x,y: x+y)\n",
    "print (contagem.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contagem.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagemFinal = (palavrasRDD\n",
    "                     .map(lambda x: (x, 1))\n",
    "                     .reduceByKey(lambda x,y: x+y)\n",
    "                 )\n",
    "print (contagemFinal.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contagemFinal.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavrasUnicas = len(palavrasRDD.distinct().collect())\n",
    "print (palavrasUnicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert palavrasUnicas==3, 'valor incorreto!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1.67\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# add é equivalente a lambda x,y: x+y\n",
    "from operator import add\n",
    "total = (contagemFinal\n",
    "             .map(lambda x: (x[1]))\n",
    "             .reduce(lambda x,y: x+y)\n",
    "         )\n",
    "media = total / float(palavrasUnicas)\n",
    "print (total)\n",
    "print (round(media, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert round(media, 2)==1.67, 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def contaPalavras(chavesRDD):\n",
    "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
    "\n",
    "    Args:\n",
    "        chavesRDD (RDD of str): An RDD consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "    \"\"\"\n",
    "    return (chavesRDD\n",
    "                .map(lambda x: (x, 1))\n",
    "                .reduceByKey(lambda x,y: x+y)\n",
    "           )\n",
    "\n",
    "print (contaPalavras(palavrasRDD).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contaPalavras(palavrasRDD).collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola quem esta ai\n",
      "sem espaco esublinhado\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "import re\n",
    "def removerPontuacao(texto):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        texto (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', texto).strip().lower()\n",
    "print (removerPontuacao('Ola, quem esta ai??!'))\n",
    "print (removerPontuacao(' Sem espaco e_sublinhado!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert removerPontuacao(' O uso de virgulas, embora permitido, nao deve contar. ')=='o uso de virgulas embora permitido nao deve contar', 'string incorreta!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já existe!\n"
     ]
    }
   ],
   "source": [
    "# Apenas execute a célula\n",
    "import os.path\n",
    "import urllib.request as urllib2\n",
    "\n",
    "#url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt' # url do livro\n",
    "\n",
    "arquivo = 'C://Data//Aula02//shakespeare.txt' # local de destino: 'Data/Aula02/shakespeare.txt'\n",
    "\n",
    "if os.path.isfile(arquivo):     # verifica se já fizemos download do arquivo\n",
    "    print ('Arquivo já existe!')\n",
    "else:\n",
    "    try:        \n",
    "        response = urllib2.urlopen(url)\n",
    "        arquivo = (response.read()).split() #ja gera uma lista de palavras\n",
    "    except IOError:\n",
    "        print ('Impossível fazer o download: {0}'.format(url))\n",
    "\n",
    "# lê o arquivo com textFile e aplica a função removerPontuacao        \n",
    "shakesRDD = (sc.textFile(arquivo, 8).map(removerPontuacao))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the project gutenberg ebook of the complete works of william shakespeare by\n",
      "1: william shakespeare\n",
      "2: \n",
      "3: this ebook is for the use of anyone anywhere at no cost and with\n",
      "4: almost no restrictions whatsoever  you may copy it give it away or\n",
      "5: reuse it under the terms of the project gutenberg license included\n",
      "6: with this ebook or online at wwwgutenbergorg\n",
      "7: \n",
      "8: this is a copyrighted project gutenberg ebook details below\n",
      "9: please follow the copyright guidelines in this file\n",
      "10: \n",
      "11: title the complete works of william shakespeare\n",
      "12: \n",
      "13: author william shakespeare\n",
      "14: \n"
     ]
    }
   ],
   "source": [
    "print ('\\n'.join(shakesRDD.zipWithIndex().map(lambda x: '{0}: {1}'.format(x[1], x[0])).take(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by'], ['william', 'shakespeare'], [], ['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with'], ['almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or']]\n",
      "124787\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "shakesPalavrasRDD = shakesRDD.map(lambda x: x.split())\n",
    "total = shakesPalavrasRDD.count()\n",
    "print (shakesPalavrasRDD.take(5))\n",
    "print (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zwaggerd', 'zounds', 'zounds', 'zounds', 'zounds']\n",
      "903705\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "shakesPalavrasRDD = shakesRDD.flatMap(lambda x: x.split())\n",
    "total = shakesPalavrasRDD.count()\n",
    "print (shakesPalavrasRDD.top(5))\n",
    "print (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert total==903705, 'valor incorreto!'\n",
    "print (\"OK\")\n",
    "assert shakesPalavrasRDD.top(5)==['zwaggerd', 'zounds', 'zounds', 'zounds', 'zounds'],'lista incorreta de palavras'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903705\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "shakesLimpoRDD = shakesPalavrasRDD.filter(lambda x: x != '')\n",
    "total = shakesLimpoRDD.count()\n",
    "print (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 27825), ('and', 26791), ('i', 20681), ('to', 19261), ('of', 18289), ('a', 14667), ('you', 13716), ('my', 12481), ('that', 11135), ('in', 11027), ('is', 9621), ('not', 8745), ('for', 8261), ('with', 8046), ('me', 7769)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "top15 = contaPalavras(shakesLimpoRDD)\n",
    "top15 = top15.takeOrdered(15, key = lambda x: -x[1])\n",
    "print (top15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert top15 == [('the', 27825), ('and', 26791), ('i', 20681), ('to', 19261), ('of', 18289), ('a', 14667), ('you', 13716), \n",
    "                 ('my', 12481), ('that', 11135), ('in', 11027), ('is', 9621), ('not', 8745), ('for', 8261), ('with', 8046), \n",
    "                 ('me', 7769)],'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vamos criar uma função pNorm que recebe como parâmetro p e retorna uma função que calcula a pNorma\n",
    "def pNorm(p):\n",
    "    \"\"\"Generates a function to calculate the p-Norm between two points.\n",
    "\n",
    "    Args:\n",
    "        p (int): The integer p.\n",
    "\n",
    "    Returns:\n",
    "        Dist: A function that calculates the p-Norm.\n",
    "    \"\"\"\n",
    "\n",
    "    def Dist(x,y):\n",
    "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
    "    return Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar uma RDD com valores numéricos\n",
    "numPointsRDD = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartPointsRDD = numPointsRDD.cartesian(numPointsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartPoints0RDD = cartPointsRDD.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartPoints1RDD = cartPointsRDD.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartPointsRDD = numPointsRDD.<COMPLETAR>\n",
    "\n",
    "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
    "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
    "cartPointsParesRDD = cartPointsRDD.<COMPLETAR>\n",
    "\n",
    "\n",
    "# Aplique um mapa para calcular a Distância Euclidiana entre os pares\n",
    "Euclid = pNorm(2)\n",
    "distRDD = cartPointsParesRDD.<COMPLETAR>\n",
    "\n",
    "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
    "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
    "statRDD = distRDD.<COMPLETAR>\n",
    "\n",
    "minv, maxv, meanv = statRDD.<COMPLETAR>, statRDD.<COMPLETAR>, statRDD.<COMPLETAR>\n",
    "print minv, maxv, meanv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (minv.round(2), maxv.round(2), meanv.round(2))==(0.0, 4.70, 3.65), 'Valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "0-dimensional array given. Array must be at least two-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-7241f0df584e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcartPoints1RDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \"\"\"\n\u001b[0;32m    506\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_makearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m     \u001b[0m_assertRankAtLeast2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m     \u001b[0m_assertNdSquareness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_assertRankAtLeast2\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             raise LinAlgError('%d-dimensional array given. Array must be '\n\u001b[1;32m--> 202\u001b[1;33m                     'at least two-dimensional' % a.ndim)\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_assertSquareness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: 0-dimensional array given. Array must be at least two-dimensional"
     ]
    }
   ],
   "source": [
    "cartPoints1RDD.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 199.0 failed 1 times, most recent failure: Lost task 0.0 in stage 199.0 (TID 809, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Cristiano Passos\\Anaconda3\\lib\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-312-b143d7ee3b88>\", line 1, in <lambda>\nAttributeError: 'tuple' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Cristiano Passos\\Anaconda3\\lib\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-312-b143d7ee3b88>\", line 1, in <lambda>\nAttributeError: 'tuple' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-312-b143d7ee3b88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartPointsRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1343\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 199.0 failed 1 times, most recent failure: Lost task 0.0 in stage 199.0 (TID 809, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Cristiano Passos\\Anaconda3\\lib\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-312-b143d7ee3b88>\", line 1, in <lambda>\nAttributeError: 'tuple' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Cristiano Passos\\Anaconda3\\lib\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-312-b143d7ee3b88>\", line 1, in <lambda>\nAttributeError: 'tuple' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "cartPointsRDD.map(lambda x: x[0].split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar uma função para calcular a distância de Hamming\n",
    "def Hamming(x,y):\n",
    "    \"\"\"Calculates the Hamming distance between two binary vectors.\n",
    "\n",
    "    Args:\n",
    "        x, y (np.array): Array of binary integers x and y.\n",
    "\n",
    "    Returns:\n",
    "        H (int): The Hamming distance between x and y.\n",
    "    \"\"\"\n",
    "    return (x!=y).sum()\n",
    "\n",
    "# Vamos criar uma função para calcular a distância de Jaccard\n",
    "def Jaccard(x,y):\n",
    "    \"\"\"Calculates the Jaccard distance between two binary vectors.\n",
    "\n",
    "    Args:\n",
    "        x, y (np.array): Array of binary integers x and y.\n",
    "\n",
    "    Returns:\n",
    "        J (int): The Jaccard distance between x and y.\n",
    "    \"\"\"\n",
    "    return (x==y).sum()/float( np.maximum(x,y).sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma RDD com valores categóricos\n",
    "catPointsRDD = sc.parallelize(enumerate([['alto', 'caro', 'azul'],\n",
    "                                         ['medio', 'caro', 'verde'],\n",
    "                                         ['alto', 'barato', 'azul'],\n",
    "                                         ['medio', 'caro', 'vermelho'],\n",
    "                                         ['baixo', 'barato', 'verde'],\n",
    "                                        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baixo': 0, 'barato': 1, 'alto': 2, 'medio': 3, 'verde': 4, 'vermelho': 5, 'caro': 6, 'azul': 7} 8\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# Crie um RDD de chaves únicas utilizando flatMap\n",
    "chavesRDD = (catPointsRDD\n",
    "                .flatMap(lambda x: x[1])\n",
    "                .distinct()\n",
    "                .collect()\n",
    "             )\n",
    "\n",
    "#catPointsRDD.flatMap(lambda x: x[1]).distinct().collect()\n",
    "\n",
    "chaves = dict((v,k) for k,v in enumerate(chavesRDD))\n",
    "nchaves = len(chaves)\n",
    "print (chaves, nchaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert chaves=={'baixo': 0, 'barato': 1, 'alto': 2, 'medio': 3, 'verde': 4, 'vermelho': 5, 'caro': 6, 'azul': 7}, 'valores incorretos!'\n",
    "print (\"OK\")\n",
    "\n",
    "assert nchaves==8, 'número de chaves incorreta'\n",
    "print (\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.])),\n",
       " (1, array([ 0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.])),\n",
       " (2, array([ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.])),\n",
       " (3, array([ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.])),\n",
       " (4, array([ 1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]))]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CreateNP(atributos,chaves):  \n",
    "    \"\"\"Binarize the categorical vector using a dictionary of keys.\n",
    "\n",
    "    Args:\n",
    "        atributos (list): List of attributes of a given object.\n",
    "        chaves (dict): dictionary with the relation attribute -> index\n",
    "\n",
    "    Returns:\n",
    "        array (np.array): Binary array of attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    array = np.zeros(len(chaves))\n",
    "    for atr in atributos:\n",
    "        array[ chaves[atr] ] = 1\n",
    "    return array\n",
    "\n",
    "# Converte o RDD para o formato binário, utilizando o dict chaves\n",
    "binRDD = catPointsRDD.map(lambda rec: (rec[0],CreateNP(rec[1], chaves)))\n",
    "binRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartBinRDD = binRDD.cartesian(binRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.])),\n",
       " (0, array([ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.])),\n",
       " (0, array([ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.])),\n",
       " (1, array([ 0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.])),\n",
       " (0, array([ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.])),\n",
       " (2, array([ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.])),\n",
       " (0, array([ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.])),\n",
       " (3, array([ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.]))]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartBinRDD.map(lambda x: x).flatMap(lambda x: x).take(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-52dee7738b8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchaves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcartBinRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'RDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartBinRDD = binRDD.<COMPLETAR>\n",
    "\n",
    "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
    "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
    "cartBinParesRDD = cartBinRDD.<COMPLETAR>\n",
    "\n",
    "\n",
    "# Aplique um mapa para calcular a Distância de Hamming e Jaccard entre os pares\n",
    "hamRDD = cartBinParesRDD.<COMPLETAR>\n",
    "jacRDD = cartBinParesRDD.<COMPLETAR>\n",
    "\n",
    "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
    "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
    "statHRDD = hamRDD.<COMPLETAR>\n",
    "statJRDD = jacRDD.<COMPLETAR>\n",
    "\n",
    "Hmin, Hmax, Hmean = statHRDD.<COMPLETAR>, statHRDD.<COMPLETAR>, statHRDD.<COMPLETAR>\n",
    "Jmin, Jmax, Jmean = statJRDD.<COMPLETAR>, statJRDD.<COMPLETAR>, statJRDD.<COMPLETAR>\n",
    "\n",
    "print \"\\t\\tMin\\tMax\\tMean\"\n",
    "print \"Hamming:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(Hmin, Hmax, Hmean )\n",
    "print \"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (Hmin.round(2), Hmax.round(2), Hmean.round(2)) == (0.00,6.00,3.52), 'valores incorretos'\n",
    "print \"OK\"\n",
    "assert (Jmin.round(2), Jmax.round(2), Jmean.round(2)) == (0.33,2.67,1.14), 'valores incorretos'\n",
    "print \"OK\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
